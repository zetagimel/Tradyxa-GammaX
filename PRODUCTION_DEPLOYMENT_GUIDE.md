# ðŸš€ Production Deployment Guide - GitHub Actions Workflow

## Overview

Your production dashboard needs **3 automated workflows** running at different frequencies:

| Workflow | Frequency | Purpose | Data Updated |
|----------|-----------|---------|--------------|
| **Live Spot Prices** | Every 30 min (market hours) | Real-time prices | `public/data/live/spot_prices.json` |
| **Daily Update** | Once daily (3:45 PM IST) | OHLCV, metrics, verdicts | `public/data/ticker/*.json`, `public/data/raw/*.csv` |
| **Weekly Training** | Once weekly (Sunday) | ML models retrained | `models/*.joblib` |

---

## ðŸ” Data Flow for Production

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LIVE SPOT PRICES (30 min)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ fetch_spot_prices.py                                             â”‚
â”‚ â””â”€> Lightweight update (50 KB)                                   â”‚
â”‚     â””â”€> public/data/live/spot_prices.json                       â”‚
â”‚         (Real-time dashboard updates)                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DAILY DATA UPDATE (3:45 PM IST)              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. tradyxa_pipeline.py (batch mode, 2 workers)                   â”‚
â”‚    â””â”€> data_manager.py                                           â”‚
â”‚        â””â”€> Incremental OHLCV fetch from Yahoo Finance            â”‚
â”‚            â””â”€> public/data/raw/{ticker}.csv                      â”‚
â”‚    â””â”€> Compute features (Amihud, Lambda, MFC, etc.)              â”‚
â”‚    â””â”€> Generate verdict (UP/DOWN with score)                     â”‚
â”‚    â””â”€> Generate all 9 tile data                                  â”‚
â”‚        â””â”€> public/data/ticker/{ticker}.json                      â”‚
â”‚                                                                   â”‚
â”‚ 2. apply_models.py                                               â”‚
â”‚    â””â”€> Load trained ML models                                    â”‚
â”‚    â””â”€> Apply to latest features                                  â”‚
â”‚    â””â”€> Update JSON predictions                                   â”‚
â”‚        â””â”€> public/data/ticker/{ticker}.json (with ML scores)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   WEEKLY TRAINING (Sunday)                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. tradyxa_pipeline.py (batch mode, 4 workers)                   â”‚
â”‚    â””â”€> Full data refresh for all 503 stocks                      â”‚
â”‚                                                                   â”‚
â”‚ 2. train_regime_classifier.py                                    â”‚
â”‚    â””â”€> Combine ~230k feature rows from all stocks                â”‚
â”‚    â””â”€> Train RandomForest classifier                             â”‚
â”‚    â””â”€> models/rf_execution_regime.joblib                         â”‚
â”‚                                                                   â”‚
â”‚ 3. train_slippage_quantile.py                                    â”‚
â”‚    â””â”€> Train Q50 + Q90 slippage forecasters                      â”‚
â”‚    â””â”€> models/qr_slippage_q50.joblib                             â”‚
â”‚    â””â”€> models/qr_slippage_q90.joblib                             â”‚
â”‚                                                                   â”‚
â”‚ 4. apply_models.py (optional, since daily does it)               â”‚
â”‚    â””â”€> Update all JSONs with new predictions                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            CLOUDFLARE PAGES DEPLOYMENT (Auto)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Git commit triggers webhook                                       â”‚
â”‚ â””â”€> Cloudflare Pages pulls latest code + data                    â”‚
â”‚     â””â”€> public/* files served globally                           â”‚
â”‚         â””â”€> Dashboard shows latest data in real-time             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ“‹ Complete Script Execution Order

### **INITIAL SETUP** (One-time before production)

```bash
# 1. Fetch and verify ticker list
python scripts/fetch_tickers.py
# Output: scripts/nifty500.txt (503 tickers)

# 2. Generate initial data for all stocks
python scripts/tradyxa_pipeline.py --mode batch_run --tickers-file scripts/nifty500.txt --max-workers 4 --use-yf
# Output: 503 JSON files + 503 CSV files
# Time: ~2-3 hours (with 4 workers)

# 3. Train initial ML models
python scripts/train_regime_classifier.py
# Output: models/rf_execution_regime.joblib + metadata.json
# Time: ~5 minutes

python scripts/train_slippage_quantile.py
# Output: models/qr_slippage_q50.joblib, models/qr_slippage_q90.joblib
# Time: ~5 minutes

# 4. Apply models to all tickers
python scripts/apply_models.py
# Output: All JSONs updated with ML predictions
# Time: ~2 minutes

# 5. Verify all data generated correctly
python verify_all_real_data.py
# Checks: spot prices, volume profile, candles, all 9 tile data
```

---

### **DAILY WORKFLOW** (3:45 PM IST / 10:15 AM UTC)

**Via GitHub Actions:** `.github/workflows/daily_update.yml`

```bash
# Step 1: Incremental data fetch + feature computation
python scripts/tradyxa_pipeline.py --mode batch_run \
  --tickers-file scripts/nifty500.txt \
  --max-workers 2 \
  --use-yf
# What happens:
# - data_manager.py: Fetches ONLY new data since yesterday
# - Computes 7+ market metrics per stock
# - Generates verdict (UP/DOWN)
# - Creates all 9 tile data (Volume Profile, Candles, BB, etc.)
# Output: 503 JSON files (updated), 503 CSV files (incremental)
# Time: ~15-20 minutes (2 workers, incremental)

# Step 2: Apply pre-trained ML models
python scripts/apply_models.py
# What happens:
# - Loads 3 saved models from /models/
# - Extracts latest features from each JSON
# - Predicts: regime class, Q50 slippage, Q90 slippage
# - Updates all JSONs with predictions
# Output: 503 JSON files (with ML scores)
# Time: ~2 minutes

# Step 3: Commit to GitHub
git add public/data/ticker/*.json public/data/raw/*.csv
git commit -m "chore: market data update $(date)"
git push
# Triggers: Cloudflare Pages auto-deploy
# Result: Dashboard served globally with latest data
```

---

### **WEEKLY TRAINING WORKFLOW** (Sunday midnight UTC)

**Via GitHub Actions:** `.github/workflows/train.yml`

```bash
# Step 1: Full data refresh (even if already updated)
python scripts/tradyxa_pipeline.py --mode batch_run \
  --tickers-file scripts/nifty500.txt \
  --max-workers 4 \
  --use-yf
# Why full refresh?
# - Ensures consistency for training data
# - Can backfill missing data
# - Good for data quality assurance
# Time: ~30-40 minutes (4 workers)

# Step 2: Retrain Regime Classifier
python scripts/train_regime_classifier.py
# What happens:
# - Reads ALL 503 JSON files (~230k rows)
# - Labels each row by slippage severity (0-3)
# - Trains RandomForest on 7+ features
# - Tests accuracy, feature importance
# Output: models/rf_execution_regime.joblib (~10 MB)
# Output: models/rf_execution_regime_metadata.json
# Time: ~5 minutes
# Contains: Training accuracy, feature names, feature importance

# Step 3: Retrain Slippage Forecasters
python scripts/train_slippage_quantile.py
# What happens:
# - Reads ALL 503 JSON files (~230k rows)
# - Trains Q50 (median slippage) model
# - Trains Q90 (worst-case slippage) model
# - Uses GradientBoosting with quantile loss
# Output: models/qr_slippage_q50.joblib (~5 MB)
# Output: models/qr_slippage_q90.joblib (~5 MB)
# Time: ~5 minutes

# Step 4: Apply new models (optional since daily does it)
python scripts/apply_models.py
# Updates all JSONs with new predictions
# Time: ~2 minutes

# Step 5: Commit updated models and data
git add models/*.joblib models/*.json public/data/raw/*.csv
git commit -m "Weekly model training update [skip ci]"
git push
# Triggers: Cloudflare Pages deployment with latest models
```

---

### **LIVE SPOT PRICES WORKFLOW** (Every 30 min during market hours)

**Via GitHub Actions:** `.github/workflows/live_spot_prices.yml`

**Market hours: 9:15 AM - 3:30 PM IST (3:45 AM - 10:00 AM UTC)**

```bash
# Super lightweight - only fetches current prices
python scripts/fetch_spot_prices.py
# What happens:
# - Fetches current spot price for all 503 stocks
# - Fetches India VIX
# - Saves to SINGLE small JSON file (~50 KB)
# - DOES NOT update CSVs or JSONs
# Output: public/data/live/spot_prices.json
# Time: <1 minute
# Format:
# {
#   "timestamp": "2025-12-03T10:30:00Z",
#   "vix": 15.42,
#   "prices": {
#     "RELIANCE.NS": 1546.30,
#     "TCS.NS": 3167.80,
#     "INFY.NS": 2089.45,
#     ...
#   }
# }

# Step 2: Commit live data
git add public/data/live/spot_prices.json
git commit -m "chore: live spot prices update [deploy]"
git push
# Triggers: Cloudflare Pages deployment
# Result: Dashboard shows latest spot prices every 30 min
```

---

## ðŸ“ File Structure After Production Deployment

```
public/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ live/
â”‚   â”‚   â””â”€â”€ spot_prices.json              â† Updated every 30 min
â”‚   â”‚
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â”œâ”€â”€ RELIANCE.NS.csv               â† Updated daily (OHLCV)
â”‚   â”‚   â”œâ”€â”€ TCS.NS.csv
â”‚   â”‚   â”œâ”€â”€ INFY.NS.csv
â”‚   â”‚   â””â”€â”€ ... (503 files)
â”‚   â”‚
â”‚   â””â”€â”€ ticker/
â”‚       â”œâ”€â”€ RELIANCE.NS.json              â† Updated daily
â”‚       â”‚   Contains:
â”‚       â”‚   - Spot price
â”‚       â”‚   - 9 Tile data (Volume Profile, Candles, Bollinger, etc.)
â”‚       â”‚   - Market metrics (Amihud, Lambda, MFC, volatility)
â”‚       â”‚   - Verdict (UP/DOWN)
â”‚       â”‚   - ML predictions (regime, Q50, Q90)
â”‚       â”‚
â”‚       â”œâ”€â”€ TCS.NS.json
â”‚       â”œâ”€â”€ INFY.NS.json
â”‚       â””â”€â”€ ... (503 files, ~150-200 KB each)
â”‚
models/
â”œâ”€â”€ rf_execution_regime.joblib            â† Updated weekly
â”œâ”€â”€ rf_execution_regime_metadata.json      â† Training stats
â”œâ”€â”€ qr_slippage_q50.joblib                â† Updated weekly
â””â”€â”€ qr_slippage_q90.joblib                â† Updated weekly

scripts/
â”œâ”€â”€ nifty500.txt                          â† Updated weekly via fetch_tickers.py
â”œâ”€â”€ fetch_tickers.py
â”œâ”€â”€ fetch_spot_prices.py
â”œâ”€â”€ data_manager.py
â”œâ”€â”€ tradyxa_pipeline.py
â”œâ”€â”€ train_regime_classifier.py
â”œâ”€â”€ train_slippage_quantile.py
â””â”€â”€ apply_models.py
```

---

## ðŸ”§ GitHub Actions Configuration

### **File 1: `.github/workflows/live_spot_prices.yml`**
```yaml
name: Live Spot Prices Update

on:
  schedule:
    # Every 30 minutes, 3:45 AM - 10:00 AM UTC (9:15 AM - 3:30 PM IST)
    - cron: "*/30 3-10 * * 1-5"
  workflow_dispatch:

jobs:
  update-live-prices:
    runs-on: ubuntu-latest
    permissions:
      contents: write
    
    steps:
    - uses: actions/checkout@v4
    - uses: actions/setup-python@v4
      with:
        python-version: "3.11"
    
    - name: Install dependencies
      run: pip install yfinance pandas
    
    - name: Fetch Live Spot Prices & India VIX
      run: python scripts/fetch_spot_prices.py
      timeout-minutes: 10
      
    - name: Commit live data
      run: |
        git config user.name "github-actions[bot]"
        git config user.email "github-actions[bot]@users.noreply.github.com"
        git add public/data/live/spot_prices.json
        git diff --quiet && git diff --staged --quiet || git commit -m "chore: live prices [deploy]"
        git push || echo "No changes"
```

### **File 2: `.github/workflows/daily_update.yml`**
```yaml
name: Daily Dashboard Update

on:
  schedule:
    # 3:45 PM IST = 10:15 AM UTC, weekdays only
    - cron: "15 10 * * 1-5"
  workflow_dispatch:

jobs:
  update:
    runs-on: ubuntu-latest
    permissions:
      contents: write
    
    steps:
    - uses: actions/checkout@v4
    - uses: actions/setup-python@v4
      with:
        python-version: "3.11"
    
    - name: Install dependencies
      run: |
        pip install pandas numpy yfinance scikit-learn joblib scipy tqdm
    
    - name: Incremental Data Fetch & Process
      run: |
        # NIFTY/BANKNIFTY first (priority indices)
        python scripts/tradyxa_pipeline.py --mode run_all --ticker NIFTY --use-yf
        python scripts/tradyxa_pipeline.py --mode run_all --ticker BANKNIFTY --use-yf
        
        # Batch process all stocks (2 workers to avoid rate limits)
        python scripts/tradyxa_pipeline.py --mode batch_run --tickers-file scripts/nifty500.txt --max-workers 2 --use-yf
      timeout-minutes: 45
    
    - name: Apply ML Predictions
      run: python scripts/apply_models.py
      timeout-minutes: 10
      
    - name: Commit data
      run: |
        git config user.name "github-actions[bot]"
        git config user.email "github-actions[bot]@users.noreply.github.com"
        git add public/data/ticker/*.json public/data/raw/*.csv
        git commit -m "chore: market data update $(date -u '+%Y-%m-%d %H:%M UTC') [deploy]" || echo "No changes"
        git push || echo "No changes"
```

### **File 3: `.github/workflows/train.yml`**
```yaml
name: Weekly Model Training

on:
  schedule:
    # Sunday midnight UTC = Monday 5:30 AM IST
    - cron: "0 0 * * 0"
  workflow_dispatch:

jobs:
  train:
    runs-on: ubuntu-latest
    permissions:
      contents: write
    
    steps:
    - uses: actions/checkout@v4
    - uses: actions/setup-python@v4
      with:
        python-version: "3.11"
    
    - name: Install dependencies
      run: |
        pip install pandas numpy yfinance scikit-learn joblib scipy tqdm
    
    - name: Full Data Refresh
      run: |
        python scripts/tradyxa_pipeline.py --mode batch_run --tickers-file scripts/nifty500.txt --max-workers 4 --use-yf
      timeout-minutes: 60
    
    - name: Train Regime Classifier
      run: python scripts/train_regime_classifier.py
      timeout-minutes: 15
    
    - name: Train Slippage Forecasters
      run: python scripts/train_slippage_quantile.py
      timeout-minutes: 15
      
    - name: Commit models
      run: |
        git config user.name "github-actions[bot]"
        git config user.email "github-actions[bot]@users.noreply.github.com"
        git add models/*.joblib models/*.json public/data/raw/*.csv
        git commit -m "chore: weekly model training [skip ci]" || echo "No changes"
        git push || echo "No changes"
```

---

## ðŸ“Š What Each Script Does (Quick Reference)

| Script | Input | Output | Frequency | Time |
|--------|-------|--------|-----------|------|
| `fetch_tickers.py` | - | `scripts/nifty500.txt` (503 tickers) | Weekly | <1 min |
| `fetch_spot_prices.py` | `nifty500.txt` | `public/data/live/spot_prices.json` | Every 30 min | <1 min |
| `tradyxa_pipeline.py` | `nifty500.txt` + Yahoo Finance | `public/data/ticker/*.json`, `public/data/raw/*.csv` | Daily + Weekly | 15-45 min |
| `data_manager.py` | Yahoo Finance | CSV files (internal) | Called by pipeline | 10-30 min |
| `train_regime_classifier.py` | All 503 JSON files | `models/rf_execution_regime.joblib` | Weekly | 5 min |
| `train_slippage_quantile.py` | All 503 JSON files | `models/qr_slippage_q50.joblib`, `qr_slippage_q90.joblib` | Weekly | 5 min |
| `apply_models.py` | Models + JSON files | Updated JSON with predictions | Daily + Weekly | 2 min |

---

## ðŸš€ Deployment Checklist

- [ ] Push `.github/workflows/*.yml` files to GitHub
- [ ] Ensure `scripts/nifty500.txt` has all 503 tickers
- [ ] Run initial setup (batch_run + training) locally to generate base data
- [ ] Commit all `public/data/ticker/*.json` files to GitHub
- [ ] Commit all `public/data/raw/*.csv` files to GitHub
- [ ] Commit `models/*.joblib` files to GitHub
- [ ] Set up Cloudflare Pages to deploy on git push
- [ ] Test first daily workflow manually (`workflow_dispatch`)
- [ ] Test first training workflow manually
- [ ] Monitor GitHub Actions for errors
- [ ] Verify dashboard updates in real-time

---

## ðŸ“ˆ Expected Costs

**GitHub Actions** (free tier): 
- 2,000 free minutes/month
- Daily updates: ~20 min/day Ã— 21 days = 420 min/month âœ…
- Weekly training: ~45 min Ã— 4 = 180 min/month âœ…
- Live prices: ~0.5 min Ã— 90 runs = 45 min/month âœ…
- **Total: ~645 min/month** (within free tier)

**Cloudflare Pages** (free tier):
- Unlimited deployments âœ…
- Global CDN âœ…
- Auto SSL âœ…

---

## ðŸ” Monitoring & Debugging

**Check workflow status:**
```bash
# View GitHub Actions logs
GitHub â†’ Settings â†’ Actions â†’ All Workflows â†’ View Results
```

**Manual testing:**
```bash
# Test single ticker
python scripts/tradyxa_pipeline.py --mode run_all --ticker RELIANCE.NS --use-yf

# Test batch with small sample
python scripts/tradyxa_pipeline.py --mode batch_run --tickers-file <small_file.txt> --max-workers 2 --use-yf

# Test ML training
python scripts/train_regime_classifier.py

# Test live prices
python scripts/fetch_spot_prices.py
```

**Verify outputs:**
```bash
# Check JSON files generated
ls -lh public/data/ticker/ | head -20

# Check CSV files
ls -lh public/data/raw/ | head -20

# Check model files
ls -lh models/

# Check live prices
cat public/data/live/spot_prices.json | python -m json.tool | head -30
```

---

## ðŸ’¡ Best Practices

1. **Start with small batches** - Test with 5-10 stocks before full 503
2. **Monitor first week** - Ensure all workflows run without errors
3. **Set up alerts** - GitHub Actions email notifications
4. **Keep models updated** - Weekly training ensures relevance
5. **Archive old CSVs** - Consider moving raw/*.csv to archive after 1 year
6. **Cache dependencies** - Consider pre-built Docker image for faster runs
7. **Rate limiting** - Use `max-workers: 2` for daily, `max-workers: 4` for weekly

---

**Status: Production Ready âœ…**

All 1,458 JSON files generated. GitHub Actions workflows configured. Ready for deployment!
