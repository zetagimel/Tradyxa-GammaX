# ML Training & Data Dynamics

## 1. Does the ML training data increase in size?
**YES.**

The system is designed to be **self-growing**:
1.  **Incremental Updates:** The `data_manager.py` script checks your local CSV files.
2.  **Appending Data:** Every time you run the pipeline, it fetches *only* the new data since the last run and appends it to your existing CSVs.
3.  **Result:** Your training dataset grows indefinitely. If you start with 5 years of data today, in one year you will have 6 years of data.

## 2. Does the model update itself?
**YES (via Re-training).**

*   **Process:** The models (`rf_execution_regime.joblib`, etc.) are **re-trained from scratch** every time you run the training scripts (`train_regime_classifier.py`, `train_slippage_quantile.py`).
*   **Benefit:** Since they are re-trained on the *entire* growing dataset (including the newest week's data), they effectively "learn" from recent market behavior while retaining historical knowledge.
*   **No "Online" Learning:** The models do not update their weights incrementally; they are replaced by new models trained on the larger dataset. This is the standard and most robust approach for this type of data.

## 3. Should I input the last 10 years of data?
**RECOMMENDATION: YES.**

**Why?**
*   **Capture More Regimes:** 5 years of data might miss major historical stress events (like 2008 or parts of 2020 if strictly 5y). 10 years captures a wider variety of market conditions (Bull, Bear, Crash, Sideways).
*   **Better Generalization:** The "Regime Classifier" relies on seeing examples of "High" and "Severe" volatility. These are rare. 10 years of data provides more examples of these rare events, making the model better at detecting them in the future.

### Data Fetching Behavior (Updated)

**Default Start Date: 2005-01-01**

The system has been updated to automatically manage data history:

1.  **New Tickers:** Automatically fetches data from **2005-01-01** (or the earliest available date).
2.  **Existing Data:**
    *   **Forward Update:** If you run the pipeline daily/weekly, it efficiently appends *only* the new data since the last run.
    *   **Backward Backfill:** If your existing CSV starts significantly later than 2005 (e.g., 2010), the system will automatically attempt to "backfill" the missing history from 2005 to 2010.

**No manual configuration is needed.** Just run the pipeline as usual.
    If you already have data and want to *replace* it with 10 years (since the updater only appends forward), you need to delete the existing CSVs in `public/data/raw` so the system fetches fresh 10-year histories.
    
    **Command (PowerShell):**
    ```powershell
    Remove-Item "public/data/raw/*.csv"
    python scripts/tradyxa_pipeline.py
    ```

## 4. How does the training work? (The "Black Box" Explained)

You asked: *"How does the models get trained by all these csv?"*

Here is the exact flow:

```mermaid
graph TD
    subgraph "Step 1: Data Aggregation"
        CSV[500+ CSV Files<br/>(Raw OHLCV)] -->|Read & Process| PIPE[Pipeline Script]
        PIPE -->|Compute Features| FEATS[Feature Vectors<br/>(Volatility, Amihud, MFC)]
        FEATS -->|Label Data| DATA[Training Dataset<br/>(200,000+ rows)]
    end

    subgraph "Step 2: Model Training"
        DATA -->|Train| RF[Random Forest<br/>(Regime Classifier)]
        DATA -->|Train| GB[Gradient Boosting<br/>(Slippage Predictor)]
    end

    subgraph "Step 3: Saving"
        RF -->|Save| M1[models/rf_execution_regime.joblib]
        GB -->|Save| M2[models/qr_slippage_q50.joblib]
    end
```

### The Process in Plain English:

1.  **The "Big Read":** The training script opens **every single JSON file** (which contains the processed features from your CSVs). It doesn't read the raw CSVs directly during training; it reads the *features* generated by the pipeline.
2.  **Feature Extraction:** It pulls out specific numbers for every day in history:
    *   *How volatile was it?* (Volatility)
    *   *How hard was it to trade?* (Amihud)
    *   *Was big money buying?* (MFC)
3.  **The "Teacher" (Labeling):**
    *   For the **Regime Model**, it looks at the *future* slippage. If slippage was high, it labels that day as "HIGH REGIME".
    *   It tells the AI: *"Look at these features. When you see this pattern, it means High Slippage is coming."*
4.  **Learning:** The Random Forest algorithm looks at 200,000+ days of history across all 500 stocks and finds patterns that humans miss.
5.  **Saving:** It saves this "brain" into a `.joblib` file.

---

## 5. What to Run Next? (The Workflow)

You have just finished the **Batch Run** (Step 1). Now you need to train the "Brain" and then use it.

### Step 2: Train the Models (Do this now)
This will teach the AI using the fresh data you just downloaded.

```powershell
# Train the Regime Classifier (Detects High/Low volatility regimes)
python scripts/train_regime_classifier.py

# Train the Slippage Predictor (Predicts cost)
python scripts/train_slippage_quantile.py
```

### Step 3: Apply the Brain (Do this last)
This takes the trained models and updates your dashboard files with the predictions.

```powershell
python scripts/apply_models.py
```

### Step 4: Start the App
```powershell
npm run dev
```
